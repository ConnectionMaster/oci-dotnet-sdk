/*
 * Copyright (c) 2020, 2025, Oracle and/or its affiliates. All rights reserved.
 * This software is dual-licensed to you under the Universal Permissive License (UPL) 1.0 as shown at https://oss.oracle.com/licenses/upl or Apache License 2.0 as shown at http://www.apache.org/licenses/LICENSE-2.0. You may choose either license.
 */

// NOTE: Code generated by OracleSDKGenerator.
// DO NOT EDIT this file manually.


using System.ComponentModel.DataAnnotations;
using System.Runtime.Serialization;
using Newtonsoft.Json;
using Newtonsoft.Json.Converters;


namespace Oci.AilanguageService.Models
{
    /// <summary>
    /// Model level named entity recognition metrics
    /// </summary>
    public class NamedEntityRecognitionModelMetrics 
    {
        
        /// <value>
        /// F1-score, is a measure of a model\u2019s accuracy on a dataset
        /// </value>
        /// <remarks>
        /// Required
        /// </remarks>
        [Required(ErrorMessage = "MicroF1 is required.")]
        [JsonProperty(PropertyName = "microF1")]
        public System.Nullable<float> MicroF1 { get; set; }
        
        /// <value>
        /// Precision refers to the number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives)
        /// </value>
        /// <remarks>
        /// Required
        /// </remarks>
        [Required(ErrorMessage = "MicroPrecision is required.")]
        [JsonProperty(PropertyName = "microPrecision")]
        public System.Nullable<float> MicroPrecision { get; set; }
        
        /// <value>
        /// Measures the model's ability to predict actual positive classes. It is the ratio between the predicted true positives and what was actually tagged. The recall metric reveals how many of the predicted classes are correct.
        /// </value>
        /// <remarks>
        /// Required
        /// </remarks>
        [Required(ErrorMessage = "MicroRecall is required.")]
        [JsonProperty(PropertyName = "microRecall")]
        public System.Nullable<float> MicroRecall { get; set; }
        
        /// <value>
        /// F1-score, is a measure of a model\u2019s accuracy on a dataset
        /// </value>
        /// <remarks>
        /// Required
        /// </remarks>
        [Required(ErrorMessage = "MacroF1 is required.")]
        [JsonProperty(PropertyName = "macroF1")]
        public System.Nullable<float> MacroF1 { get; set; }
        
        /// <value>
        /// Precision refers to the number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives)
        /// </value>
        /// <remarks>
        /// Required
        /// </remarks>
        [Required(ErrorMessage = "MacroPrecision is required.")]
        [JsonProperty(PropertyName = "macroPrecision")]
        public System.Nullable<float> MacroPrecision { get; set; }
        
        /// <value>
        /// Measures the model's ability to predict actual positive classes. It is the ratio between the predicted true positives and what was actually tagged. The recall metric reveals how many of the predicted classes are correct.
        /// </value>
        /// <remarks>
        /// Required
        /// </remarks>
        [Required(ErrorMessage = "MacroRecall is required.")]
        [JsonProperty(PropertyName = "macroRecall")]
        public System.Nullable<float> MacroRecall { get; set; }
        
        /// <value>
        /// F1-score, is a measure of a model\u2019s accuracy on a dataset
        /// </value>
        [JsonProperty(PropertyName = "weightedF1")]
        public System.Nullable<float> WeightedF1 { get; set; }
        
        /// <value>
        /// Precision refers to the number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives)
        /// </value>
        [JsonProperty(PropertyName = "weightedPrecision")]
        public System.Nullable<float> WeightedPrecision { get; set; }
        
        /// <value>
        /// Measures the model's ability to predict actual positive classes. It is the ratio between the predicted true positives and what was actually tagged. The recall metric reveals how many of the predicted classes are correct.
        /// </value>
        [JsonProperty(PropertyName = "weightedRecall")]
        public System.Nullable<float> WeightedRecall { get; set; }
        
    }
}
